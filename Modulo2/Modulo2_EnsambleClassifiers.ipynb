{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dab0e51",
   "metadata": {},
   "source": [
    "# Ensamble Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0975f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pydotplus\n",
    "from sklearn import tree\n",
    "from IPython.display import Image\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e77ad",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9712a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"X_train.txt\", header=None,  delim_whitespace=True)\n",
    "y_train = pd.read_csv(\"y_train.txt\", header=None, delim_whitespace=True )\n",
    "X_test = pd.read_csv(\"X_test.txt\", header=None, delim_whitespace=True )\n",
    "y_test = pd.read_csv(\"y_test.txt\", header=None, delim_whitespace=True )\n",
    "#subject_test = pd.read_csv(\"subject_test.txt\", header=None, delim_whitespace=True )\n",
    "subject_train = pd.read_csv(\"subject_train.txt\", header=None, delim_whitespace=True )\n",
    "features = pd.read_csv(\"features.txt\", header=None, delim_whitespace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf6318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = features\n",
    "feature.drop(0,inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fee578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inseriamo l'intestazione al dataset\n",
    "lista=[]\n",
    "feat_transpa = feature.transpose()\n",
    "for i in range(561):\n",
    "    lista.append(feat_transpa.iloc[0][i])\n",
    "X_test.columns=lista    \n",
    "X_train.columns=lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f527fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminiamo le colonne che contengono la stima mad in quanto quasi uguale a dev.std\n",
    "stringa=\"mad()\"\n",
    "for col in X_train.columns:\n",
    "    if(stringa in col):\n",
    "        X_train.drop(labels=col, axis=1, inplace=True)\n",
    "for col in X_test.columns:\n",
    "    if(stringa in col):\n",
    "        X_test.drop(labels=col, axis=1, inplace=True)\n",
    "#for col in features:\n",
    "#    if (stringa in col):\n",
    "#        features.drop(labels=col,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3b9906",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e1db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f70a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=None, min_samples_leaf= 1, min_samples_split=4,\n",
    "                             n_estimators=400, max_features='log2', n_jobs=-1, criterion ='entropy')\n",
    "clf.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbeca0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "nbr_features = 15\n",
    "\n",
    "tree_feature_importances = clf.feature_importances_\n",
    "sorted_idx = tree_feature_importances.argsort()[-nbr_features:]\n",
    "\n",
    "y_ticks = np.arange(0, len(sorted_idx))\n",
    "fig, ax = plt.subplots()\n",
    "plt.barh(y_ticks, tree_feature_importances[sorted_idx])\n",
    "plt.yticks(y_ticks, np.array(X_train.columns)[sorted_idx])\n",
    "plt.title(\"Random Forest Feature Importances (MDI)\", size=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3653adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Albero 0\n",
    "dot_data = tree.export_graphviz(clf.estimators_[0], out_file=None,  \n",
    "                                feature_names=X_train.columns, \n",
    "                                class_names=[\"WALKING\", \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\", \"SITTING\", \"STANDING\", \"LAYING\" ],  \n",
    "                                filled=True, rounded=True,  \n",
    "                                special_characters=True, max_depth=4)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5287637",
   "metadata": {},
   "source": [
    "### Tuning the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d19dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3268301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_list = {'max_depth': [None],    \n",
    "             'min_samples_split': [2, 4, 7, 10, 15],\n",
    "             'min_samples_leaf': [1, 2, 3, 5, 10], \n",
    "             'criterion': ['entropy'],\n",
    "             'max_features': ['auto', 'sqrt', 'log2'],\n",
    "             'n_estimators' : [5,20,50,100,200,300,400]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca66451",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(clf, param_grid=param_list, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, np.ravel(y_train))\n",
    "clf_gs = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gs = grid_search.best_estimator_\n",
    "clf_gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03efb967",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0482e1d4",
   "metadata": {},
   "source": [
    "### Roc Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e6e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from yellowbrick import ROCAUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa4d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binarize FARE ATTENZIONE PERCHE' DOPO SBALLA LE CLASSI\n",
    "y_test1 = label_binarize(y_test, classes=[1,2,3,4,5,6])\n",
    "y_train1 = label_binarize(y_train, classes=[1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b0416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn to predict each class against the other\n",
    "classifier = OneVsRestClassifier(clf)   \n",
    "classifier.fit(X_train, y_train1)\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred_proba = classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfe9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "CLASS_LABELS = [ \"WALKING\" , \"WALKING_UPSTAIRS\", \"WALKING_DOWNSTAIRS\", \"SITTING\", \"STANDING\", \"LAYING\"]\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(len(CLASS_LABELS)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test1[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e2ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot\n",
    "lw=2\n",
    "colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\",\"navy\", \"deeppink\", \"gold\"])\n",
    "for i, color in zip(range(len(CLASS_LABELS)), colors):\n",
    "    plt.plot(\n",
    "        fpr[i],\n",
    "        tpr[i],\n",
    "        color=color,\n",
    "        lw=lw,\n",
    "        label=\"ROC curve of class {0} (area = {1:0.2f})\".format(i+1, roc_auc[i]),\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", lw=lw)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC curve multiclass Random Forest\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee40c8e",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e919a482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# Base estimator con DecTree, SVC o Rand forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64e9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, random_state=0)\n",
    "clf1.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "y_pred = clf1.predict(X_test)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32975f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BaggingClassifier(base_estimator=SVC(C=1000), n_estimators=100, random_state=0, n_jobs=-1)\n",
    "clf.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2420163",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ee630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa8b06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=0, n_jobs= -1)\n",
    "clf.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a25245",
   "metadata": {},
   "source": [
    "### Tuning the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d4aa21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GS adaboostclassif\n",
    "abc = AdaBoostClassifier(base_estimator=RandomForestClassifier(max_depth=None, min_samples_leaf= 1, min_samples_split=4,\n",
    "                                                               n_estimators=400, max_features='log2'), random_state=1)\n",
    "\n",
    "parameters = {'n_estimators':list(range(40, 400, 20)), \n",
    "              'learning_rate':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]}\n",
    "\n",
    "gs = GridSearchCV(abc, parameters, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs.fit(X_train, np.ravel(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf86070",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal hyperparameter combination:\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b504f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(random_state=42), n_estimators=220, learning_rate = 0.1, random_state=0)\n",
    "clf.fit(X_train, np.ravel(y_train))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c65377",
   "metadata": {},
   "source": [
    "##  Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FEATURE IMPORTANCES + CLASSIFICATION REPORT\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "baseline = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100,max_depth=3, min_samples_split=2, min_samples_leaf=1, subsample=1,max_features='sqrt', random_state=10)\n",
    "baseline.fit(X_train,np.ravel(y_train[0]))\n",
    "predictors=list(X_train)\n",
    "feat_imp = pd.Series(baseline.feature_importances_, predictors).sort_values(ascending=False)\n",
    "feat_imp[:10].plot(kind='barh', title='Importance of Features')\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "pred=baseline.predict(X_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f046fb6",
   "metadata": {},
   "source": [
    "### Tuning the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627e4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { 'max_depth': [3,6,10],\n",
    "           'learning_rate': [0.01, 0.05, 0.1],\n",
    "           'n_estimators': [100, 500],\n",
    "          'max_features': ['auto', 'sqrt', 'log2']\n",
    "           }\n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=100, random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7729717d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid_search = GridSearchCV(model, param_grid=params,\n",
    "                                 n_jobs=-1, cv=3)\n",
    "model_grid_search.fit(X_train, np.ravel(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a1024",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a119f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.1, max_depth=3, max_features='log2', random_state=0)\n",
    "clf.fit(X_train, y_train[0])\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5429b1b",
   "metadata": {},
   "source": [
    "## HistGradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb162b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model=HistGradientBoostingClassifier(random_state=42, max_leaf_nodes=4)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_leaf_nodes': [3,10,15,30],\n",
    "    'max_depth' : [3,6,10],\n",
    "    'max_bins' : [50,100,150]\n",
    "}\n",
    "model_grid_search = GridSearchCV(model, param_grid=param_grid,\n",
    "                                 n_jobs=-1, cv=3)\n",
    "model_grid_search.fit(X_train, np.ravel(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da98f7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ba2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = HistGradientBoostingClassifier(max_iter=100, learning_rate=0.1,\n",
    "                                     max_depth=3, max_bins=150, # max\n",
    "                                     random_state=0, max_leaf_nodes=3, loss='categorical_crossentropy')\n",
    "clf.fit(X_train, np.ravel(y_train[0]))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test, y_pred))\n",
    "print('F1-score %s' % f1_score(y_test, y_pred, average=None))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318ba0d7",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c36fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10860467",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708412cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "#TUNING ATTENZIONE CHE LE LABEL DEVONO PARTIRE DA 0 PER QUESTO USIAMO Y_TEST E TRAIN [\"NEW\"]\n",
    "\n",
    "#learning_rate [0.05, 0.1]\n",
    "#max_depth= [3,6,10]\n",
    "#min_child_weight = 1, 3, 6\n",
    "#gamma = 0 : A smaller value like 0.1-0.2 can also be chosen for starting.\n",
    "#colsample_bytree = 0.8 : This is a commonly used used start value. Typical values range between 0.5-0.9.\n",
    "##DOPO AVER IMPOSTATO QUESTI PARAMETRI PER TUNING VEDIAMO DI REGOLARIZZARE L'OVERFITTING\n",
    "\n",
    "#reg_alpha [1e-5, 1e-2, 0.1, 1, 100] Nel nostro caso lasciandolo di default\n",
    "\n",
    "#objective='multi:softmax' to multiclass prediction\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(booster = 'gbtree', random_state=0, objective='multi:softmax',\n",
    "                              gamma = 0.1, max_depth=3,  min_child_weight=1,\n",
    "                              use_label_encoder=False, eta=0.4)\n",
    "\n",
    "xgb_model.fit(X_train, np.ravel(y_train[\"new\"]), eval_metric='mlogloss')\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "#Check overfitting\n",
    "#print('Training set score: {:.4f}'.format(xgb_model.score(X_train, y_train[\"new\"])))\n",
    "#print('Test set score: {:.4f}'.format(xgb_model.score(X_test, y_test[\"new\"])))\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test[\"new\"], y_pred))\n",
    "print('F1-score %s' % f1_score(y_test[\"new\"], y_pred, average=None))\n",
    "print(classification_report(y_test[\"new\"], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c11ff26",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c93548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "#bisogna levare caretteri speciali dai nomi delle features altrimenti si rompe\n",
    "import re\n",
    "X_train = X_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "X_test = X_test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aad82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LGBMClassifier(boosting_type='gbdt',  #'goss', #'dart'\n",
    "                     max_depth=-1, # no limit\n",
    "                     num_leaves=31,\n",
    "                     n_estimators=100,\n",
    "                     subsample_for_bin=200000,\n",
    "                     objective='multiclass',\n",
    "                     reg_alpha=0.0, #L1 regularization term on weights\n",
    "                     reg_lambda=0.0, #L2 regularization term on weights\n",
    "                     random_state=42\n",
    "                   )\n",
    "clf.fit(X_train, np.ravel(y_train[\"new\"]))\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test[\"new\"], y_pred))\n",
    "print('F1-score %s' % f1_score(y_test[\"new\"], y_pred, average=None))\n",
    "print(classification_report(y_test[\"new\"], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40d6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "clf = LGBMClassifier()\n",
    "clf.fit(X_train, y_train[0])\n",
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eecdf69",
   "metadata": {},
   "source": [
    "### Tuning the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f1247",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 num_leaves : This is the main parameter to control the complexity of the tree model. Ideally, the value of num_leaves should be less than or equal to 2^(max_depth). Value more than this will result in overfitting.\n",
    "\n",
    "#2 min_data_in_leaf : Setting it to a large value can avoid growing too deep a tree, but may cause under-fitting. In practice, setting it to hundreds or thousands is enough for a large dataset.\n",
    "\n",
    "#3 max_depth : We also can use max_depth to limit the tree depth explicitly.\n",
    "\n",
    "#FOR FASTER SPEED\n",
    "#Use bagging by setting bagging_fraction and bagging_freq.\n",
    "#Use feature sub-sampling by setting feature_fraction.\n",
    "#Use small max_bin.\n",
    "#Use save_binary to speed up data loading in future learning.\n",
    "\n",
    "#FOR BETTER ACCURACY\n",
    "#Use large max_bin (may be slower).\n",
    "#Use small learning_rate with large num_iterations\n",
    "#Use large num_leaves(may cause over-fitting)\n",
    "#Use bigger training data\n",
    "#Try dart\n",
    "#Try to use categorical feature directly\n",
    "\n",
    "#TO DEAL WITH OVER FITTING\n",
    "#Use small max_bin\n",
    "#Use small num_leaves\n",
    "#Use min_data_in_leaf and min_sum_hessian_in_leaf\n",
    "#Use bagging by set bagging_fraction and bagging_freq\n",
    "#Use feature sub-sampling by set feature_fraction\n",
    "#Use bigger training data\n",
    "#Try lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "#Try max_depth to avoid growing deep tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3859d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit_optimize\n",
    "\n",
    "params = {\n",
    "          'boosting_type': 'gbdt',\n",
    "          'objective': 'multiclass',\n",
    "          'metric': 'multi_logloss',\n",
    "          'num_class':6,\n",
    "          'max_depth':3,\n",
    "          'learning_rate': 0.2,\n",
    "          'n_estimators':150,\n",
    "          'max_bins' : 8,\n",
    "          'min_split_gain' : 0.2,\n",
    "          'min_child_samples':600\n",
    "         ## REDUCE OVERFITTING\n",
    "         #'min_data_in_leaf':[50], #primo parametro per ridurre overifitting\n",
    "         #'colsample_bytree': [0.0], #altri 3 parametri per ridurre overfitt\n",
    "         #'min_split_gain' : [0.0],\n",
    "         #'subsample' : [1],\n",
    "         #'reg_lambda' : 0.5,\n",
    "         #'reg_alpha': 1, # fattore di regolarizzazione semper per overfitting\n",
    "         }\n",
    "          #quando non c'è molta differenza di score tra\n",
    "          #train e test, we can adjust the max_depth and num_leaves parameter to reduce overfitting.\n",
    "           \n",
    "\n",
    "clf = LGBMClassifier(**params)\n",
    "\n",
    "clf.fit(X_train, y_train[0])\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "#Check OVERFITTING --> SE l'accuratezza del modello differisce di molto dal test rispetto al train siamo in overfitting\n",
    "#print('Training set score: {:.4f}'.format(clf.score(X_train, y_train[0])))\n",
    "#print('Test set score: {:.4f}'.format(clf.score(X_test, y_test[0])))\n",
    "\n",
    "print('Accuracy %s' % accuracy_score(y_test[0], y_pred))\n",
    "print('F1-score %s' % f1_score(y_test[0], y_pred, average=None))\n",
    "print (classification_report(y_test[0], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fe9446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLOT CHECK OVERFITTING (in questo caso lo vediamo al variare della depth, ma possiamo impostare un qualsiasi valore)\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# define lists to collect scores\n",
    "#rain_scores, test_scores = list(), list()\n",
    "# define the tree depths to evaluate\n",
    "#alues = [i for i in range(1, 21)]\n",
    "# evaluate a decision tree for each depth\n",
    "#or i in values:\n",
    "#   # configure the model\n",
    "#   model = LGBMClassifier(max_depth=i)\n",
    "#   # fit model on the training dataset\n",
    "#   model.fit(X_train, np.ravel(y_train[0]))\n",
    "#   # evaluate on the train dataset\n",
    "#   train_yhat = model.predict(X_train)\n",
    "#   train_acc = accuracy_score(y_train[0], train_yhat)\n",
    "#   train_scores.append(train_acc)\n",
    "#   # evaluate on the test dataset\n",
    "#   test_yhat = model.predict(X_test)\n",
    "#   test_acc = accuracy_score(y_test[0], test_yhat)\n",
    "#   test_scores.append(test_acc)\n",
    "#   # summarize progress\n",
    "#   print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc))\n",
    "#   # pot of train and test scores vs tree depth\n",
    "pyplot.plot(values, train_scores, '-o', label='Train')\n",
    "pyplot.plot(values, test_scores, '-o', label='Test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
